{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Satisfaction Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'creat_engine' from 'sqlalchemy' (C:\\Users\\gezahegne.wondachew\\AppData\\Local\\anaconda3\\lib\\site-packages\\sqlalchemy\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m creat_engine\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'creat_engine' from 'sqlalchemy' (C:\\Users\\gezahegne.wondachew\\AppData\\Local\\anaconda3\\lib\\site-packages\\sqlalchemy\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "from sqlalchemy import create_engine\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import zscore\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import creat_engine\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"expand_frame_repr\", False)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
    "from df_outlier import DfOutlier\n",
    "from vis_seaborn import *\n",
    "from vis_plotly import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/my_clean_data.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_engagements = pd.read_csv(\"../data/user_engagements.csv\")\n",
    "user_engagements.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_experiance = pd.read_csv(\"../data/TellCo_user_experience_data.csv\")\n",
    "user_experiance.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Engagement score to each user. Consider the engagement score as the Euclidean distance between the user data point & the less engaged cluster(use the first clustering for this)(Euclidean Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/TellCo_user_engagement.pkl\", \"rb\") as f:\n",
    "    kmeans1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_engagement = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Distance between the centroid and samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_df = user_engagements.set_index('MSISDN/Number')[\n",
    "    ['time_duration', 'Total Data Volume (Bytes)', 'user_sessions']]\n",
    "scaler = StandardScaler()\n",
    "scaled_array = scaler.fit_transform(eng_df)\n",
    "pd.DataFrame(scaled_array).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = normalize(scaled_array)\n",
    "pd.DataFrame(data_normalized).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = kmeans1.fit_transform(data_normalized)\n",
    "distance_from_less_engagement = list(\n",
    "    map(lambda x: x[less_engagement], distance))\n",
    "user_engagements['engagement_score'] = distance_from_less_engagement\n",
    "user_engagements.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Considering the experience score as the Euclidean distance between the user data point & the worst experience’s cluster members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/TellCo_user_experiance.pkl\", \"rb\") as f:\n",
    "    kmeans2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_experiance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = user_experiance.set_index('MSISDN/Number')[\n",
    "    ['total_avg_rtt', 'total_avg_tp', 'total_avg_tcp']]\n",
    "scaler = StandardScaler()\n",
    "scaled_array = scaler.fit_transform(exp_df)\n",
    "pd.DataFrame(scaled_array).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = normalize(scaled_array)\n",
    "pd.DataFrame(data_normalized).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = user_experiance.set_index('MSISDN/Number')\n",
    "distance = kmeans2.fit_transform(data_normalized)\n",
    "distance_from_worest_experiance = list(\n",
    "    map(lambda x: x[worst_experiance], distance))\n",
    "user_experiance['experience_score'] = distance_from_worest_experiance\n",
    "user_experiance.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the average of both engagement & experience scores as the satisfaction score & report the top 10 satisfied customer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_engagement = user_engagements['MSISDN/Number'].values\n",
    "user_id_experiance = user_experiance['MSISDN/Number'].values\n",
    "user_intersection = list(\n",
    "    set(user_id_engagement).intersection(user_id_experiance))\n",
    "user_intersection[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_engagement_df = user_engagements[user_engagements['MSISDN/Number'].isin(\n",
    "    user_intersection)]\n",
    "user_engagement_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_experiance_df = user_experiance[user_experiance['MSISDN/Number'].isin(\n",
    "    user_intersection)]\n",
    "user_experiance_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.merge(user_engagement_df, user_experiance_df, on='MSISDN/Number')\n",
    "user_df['satisfaction_score'] = (\n",
    "    user_df['engagement_score'] + user_df['experience_score'])/2\n",
    "user_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_score_df = user_df[['MSISDN/Number', 'engagement_score',\n",
    "                        'experience_score', 'satisfaction_score']]\n",
    "sat_score_df = sat_score_df.set_index('MSISDN/Number')\n",
    "sat_score_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_satisfaction = sat_score_df.sort_values(\n",
    "    'satisfaction_score', ascending=False)\n",
    "sat_top_10 = sorted_by_satisfaction['satisfaction_score'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(sat_top_10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a regression model of your choice to predict the satisfaction score of a customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(sat_score_df, 'engagement_score',\n",
    "        'experience_score', 'satisfaction_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can clearly see whene expirience score and engament score increase, \n",
    "satisfaction score will also increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sat_score_df[['engagement_score', 'experience_score']]\n",
    "y = sat_score_df[['satisfaction_score']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression()\n",
    "model = linear_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficients: \\n', model.coef_)\n",
    "print(\"Mean squared error: %.2f\" %\n",
    "      np.mean((model.predict(X_test) - y_test) ** 2))\n",
    "print('Variance score: %.2f' % model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a k-means(k=2) on the engagement & the experience score .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_satisfaction_df = user_df[[\n",
    "    'MSISDN/Number', \n",
    "    'engagement_score',\n",
    "    'experience_score']].copy()\n",
    "user_satisfaction_df = user_satisfaction_df.set_index('MSISDN/Number')\n",
    "user_satisfaction_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_satisfaction_df.plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_array = scaler.fit_transform(user_satisfaction_df)\n",
    "scaled_array\n",
    "pd.DataFrame(scaled_array).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = normalize(scaled_array)\n",
    "pd.DataFrame(data_normalized).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(data_normalized)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_satisfaction_df.insert(0, 'cluster', kmeans.labels_)\n",
    "user_satisfaction_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_satisfaction_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(user_satisfaction_df, x='engagement_score', y=\"experience_score\",\n",
    "                 color='cluster')\n",
    "Image(pio.to_image(fig, format='png', width=1200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_satisfaction_df.to_csv('../data/TellCo_user_satisfaction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the average satisfaction & experience score per cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_satisfaction_df.groupby('cluster').agg(\n",
    "    {'engagement_score': 'sum', 'experience_score': 'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 1 has higher Engagement and satisfaction score. \n",
    "Cluster 2 has vert low expirience score but higher engagement score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export your final table containing all user id + engagement, experience & satisfaction scores in your local MySQL database. Report a screenshot of a select query output on the exported table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('mysql+pymysql://root:2203@localhost/telecom_user_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('writing to the database')\n",
    "    frame = sat_score_df.to_sql(\n",
    "        \"telecom_user_analytics\", con=engine, if_exists='replace')\n",
    "    print('Writing Done!')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error writing to database: \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_sql(\"select * from telecom_user_db.telecom_user_analytics\", engine)\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model deployment tracking - deploy the model and monitor your model. Here you can use MlOps tools which can help you to track your model’s change.  Your model tracking report includes code version, start and end time, source, parameters, metrics(loss convergence) and artifacts or any output file regarding each specific run. (CSV file, screenshot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
